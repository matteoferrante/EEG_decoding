{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "207ca261-6266-4a84-bd1c-151eb2084bb6",
   "metadata": {},
   "source": [
    "## Knowledge distillation from CLIP\n",
    "\n",
    "- [ ] Obtain data\n",
    "- [ ] Transform eeg into spectrogram\n",
    "- [ ] Extract CLIP features from images\n",
    "- [ ] Train a classifier from CLIP features\n",
    "- [ ] Build a CNN classifier from images\n",
    "- [ ] Do knowledge distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c79a2b3-f907-4a10-8c66-3ca87b147bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f6da84-83c7-4900-bf28-c3a43a84c2d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import os \n",
    "from os.path import join as opj\n",
    "from datasets import load_dataset\n",
    "import tqdm\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "from pytorch_lightning import Trainer\n",
    "from versatile_diffusion_dual_guided_fake_images import *\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "from torchvision import transforms\n",
    "from scipy.signal import stft\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import classification_report\n",
    "from monai.networks import nets\n",
    "import wandb\n",
    "import pandas as pd\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "import copy\n",
    "import seaborn as sns\n",
    "# from pl_bolts.models.autoencoders.components import (\n",
    "#     resnet18_decoder,\n",
    "#     resnet18_encoder,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf79fa88-ae9a-43c8-93b3-90c1c2257759",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50eccf6-5eca-4856-ba85-0b0a1444b6d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config={\"method\": \"spectrogram\"}\n",
    "\n",
    "wandb.init(project=\"EEG_Decoding\", config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c842bf9e-dd8f-4f84-a07f-d5ebe52369a5",
   "metadata": {},
   "source": [
    "## 0. Obtain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8f9f18-be8c-44aa-9fc9-e81b86b5a2e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_dir=\"/home/matteo/data/EEG/ImageNetEEG\"\n",
    "imagenet_dir=\"/home/matteo/data/ImageNet/ILSVRC/Data/CLS-LOC/\"\n",
    "\n",
    "file=opj(dataset_dir,\"eeg_14_70_std.pth\")\n",
    "data=torch.load(file)\n",
    "\n",
    "block=torch.load(opj(dataset_dir,\"block_splits_by_image_all.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c260c5d-d9f2-42c2-91ad-8c9765077c70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eeg_train=[]\n",
    "eeg_val=[]\n",
    "eeg_test=[]\n",
    "\n",
    "subject_train=[]\n",
    "subject_val=[]\n",
    "subject_test=[]\n",
    "\n",
    "\n",
    "img_train=[]\n",
    "img_val=[]\n",
    "img_test=[]\n",
    "\n",
    "label_train=[]\n",
    "label_val=[]\n",
    "label_test=[]\n",
    "\n",
    "start=40\n",
    "stop=480\n",
    "\n",
    "for i in range(11965):\n",
    "    eeg=data[\"dataset\"][i][\"eeg\"][:,start:stop]\n",
    "    img_idx=data[\"dataset\"][i][\"image\"]\n",
    "    img_name=data[\"images\"][img_idx]\n",
    "    label=data[\"dataset\"][i][\"label\"]\n",
    "    subject=data[\"dataset\"][i][\"subject\"]\n",
    "    \n",
    "    if i in block[\"splits\"][0][\"train\"]:\n",
    "        eeg_train.append(eeg)\n",
    "        img_train.append(img_name)\n",
    "        label_train.append(label)\n",
    "        subject_train.append(subject)\n",
    "        \n",
    "    elif i in block[\"splits\"][0][\"val\"]:\n",
    "        eeg_val.append(eeg)\n",
    "        img_val.append(img_name)\n",
    "        label_val.append(label)\n",
    "        subject_val.append(subject)\n",
    "        \n",
    "    elif i in block[\"splits\"][0][\"test\"]:\n",
    "        eeg_test.append(eeg)\n",
    "        img_test.append(img_name)\n",
    "        label_test.append(label)\n",
    "        subject_test.append(subject)\n",
    "        \n",
    "    \n",
    "eeg_train=torch.stack(eeg_train,0)\n",
    "eeg_val=torch.stack(eeg_val,0)\n",
    "eeg_test=torch.stack(eeg_test,0)\n",
    "\n",
    "img_train = [opj(imagenet_dir,\"train\",i.split(\"_\")[0],f\"{i}.JPEG\") for i in img_train]\n",
    "img_val = [opj(imagenet_dir,\"train\",i.split(\"_\")[0],f\"{i}.JPEG\") for i in img_val]\n",
    "img_test = [opj(imagenet_dir,\"train\",i.split(\"_\")[0],f\"{i}.JPEG\") for i in img_test]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7545460-bddd-4e51-85c5-9dc360f8224c",
   "metadata": {},
   "source": [
    "## 1. Transform EEG into spectrogram images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395c396e-f87c-4675-a552-f1f7d2d2245e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spectro_train=[]\n",
    "spectro_val=[]\n",
    "spectro_test=[]\n",
    "\n",
    "for eeg in tqdm.tqdm(eeg_train,0):\n",
    "    f, t, Zxx = stft(eeg, 1000, nperseg=40)\n",
    "    spectro_train.append(torch.tensor(np.abs(Zxx)))\n",
    "                         \n",
    "for eeg in tqdm.tqdm(eeg_val,0):\n",
    "    f, t, Zxx = stft(eeg, 1000, nperseg=40)\n",
    "    spectro_val.append(torch.tensor(np.abs(Zxx)))\n",
    "    \n",
    "for eeg in tqdm.tqdm(eeg_test,0):\n",
    "    f, t, Zxx = stft(eeg, 1000, nperseg=40)\n",
    "    spectro_test.append(torch.tensor(np.abs(Zxx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95ae6ec-7dba-4209-b5d9-9efdbbe5cf31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spectro_train=torch.stack(spectro_train)\n",
    "spectro_val=torch.stack(spectro_val)\n",
    "spectro_test=torch.stack(spectro_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f798d5-a9be-457a-964a-18c43234ad50",
   "metadata": {},
   "source": [
    "## 2. Extract CLIP features from images \n",
    "Also extract ResNet50 represntation as backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9864866c-dace-4e35-9807-415a7b1b434f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device=\"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1c5249-1153-45f7-8c1c-c9fd4989bd04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe_embed = VersatileDiffusionDualGuidedFromCLIPEmbeddingPipeline.from_pretrained(\"shi-labs/versatile-diffusion\", )\n",
    "\n",
    "pipe_embed.remove_unused_weights()\n",
    "pipe_embed = pipe_embed.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1deeba4-b0a5-4a18-aa89-673a01953575",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resnet50 = torchvision.models.resnet50(pretrained=True)\n",
    "# Set the model to evaluation mode\n",
    "resnet50.eval()\n",
    "resnet50=resnet50.to(device)\n",
    "\n",
    "# Define the image transformations to be applied to each input image\n",
    "transform_res = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "resnet_feature_extractor = create_feature_extractor(resnet50, return_nodes=[\"avgpool\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b76681-1e50-4c14-866f-d2686651e91e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce79e06-6171-4782-a39f-c3f5906233ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_clip_img_embeds=[]\n",
    "train_resnet=[]\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm.tqdm(range(0,len(eeg_train),batch)):\n",
    "\n",
    "        #save img data\n",
    "        y= img_train[i:i+batch]\n",
    "                         \n",
    "        images=[Image.open(i).convert(\"RGB\") for i in y]\n",
    "\n",
    "        #encode images in CLIP\n",
    "        image_features=pipe_embed._encode_image_prompt(images,device=device,num_images_per_prompt=1,do_classifier_free_guidance=False).cpu()\n",
    "        train_clip_img_embeds.append(image_features)\n",
    "\n",
    "                \n",
    "        tr_imgs=torch.stack([transform_res(i) for i in images]).to(device)\n",
    "        \n",
    "        out=resnet_feature_extractor(tr_imgs)\n",
    "        train_resnet.append(out[\"avgpool\"].cpu())\n",
    "        \n",
    "        \n",
    "    train_clip_img_embeds = torch.cat(train_clip_img_embeds,axis=0)\n",
    "    train_resnet = torch.cat(train_resnet,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0321a3f0-dbd7-4f51-8665-f293d34104a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_clip_img_embeds=[]\n",
    "val_resnet=[]\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm.tqdm(range(0,len(eeg_val),batch)):\n",
    "\n",
    "        #save img data\n",
    "        y= img_val[i:i+batch]\n",
    "                         \n",
    "        images=[Image.open(i).convert(\"RGB\") for i in y]\n",
    "\n",
    "        #encode images in CLIP\n",
    "        image_features=pipe_embed._encode_image_prompt(images,device=device,num_images_per_prompt=1,do_classifier_free_guidance=False).cpu()\n",
    "        val_clip_img_embeds.append(image_features)\n",
    "\n",
    "                \n",
    "        tr_imgs=torch.stack([transform_res(i) for i in images]).to(device)\n",
    "        \n",
    "        out=resnet_feature_extractor(tr_imgs)\n",
    "        val_resnet.append(out[\"avgpool\"].cpu())\n",
    "        \n",
    "        \n",
    "    val_clip_img_embeds = torch.cat(val_clip_img_embeds,axis=0)\n",
    "    val_resnet = torch.cat(val_resnet,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f16d89-a222-4655-b94e-5322358ff6e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_clip_img_embeds=[]\n",
    "test_resnet=[]\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm.tqdm(range(0,len(eeg_test),batch)):\n",
    "\n",
    "        #save img data\n",
    "        y= img_test[i:i+batch]\n",
    "                         \n",
    "        images=[Image.open(i).convert(\"RGB\") for i in y]\n",
    "\n",
    "        #encode images in CLIP\n",
    "        image_features=pipe_embed._encode_image_prompt(images,device=device,num_images_per_prompt=1,do_classifier_free_guidance=False).cpu()\n",
    "        test_clip_img_embeds.append(image_features)\n",
    "\n",
    "                \n",
    "        tr_imgs=torch.stack([transform_res(i) for i in images]).to(device)\n",
    "        \n",
    "        out=resnet_feature_extractor(tr_imgs)\n",
    "        test_resnet.append(out[\"avgpool\"].cpu())\n",
    "        \n",
    "        \n",
    "    test_clip_img_embeds = torch.cat(test_clip_img_embeds,axis=0)\n",
    "    test_resnet = torch.cat(test_resnet,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5e097e-b64a-4080-8c89-99e3fa6f5e5f",
   "metadata": {},
   "source": [
    "## 3. Train a CLIP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f5778d-7af3-43c1-9c89-c1b63378518f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_clip_img_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663ced2b-a259-4850-8efb-25f5a962f900",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = RidgeClassifier().fit(train_clip_img_embeds[:,0,:].numpy(), label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310f2df0-b8b6-4f13-bf44-a175ef650895",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf.score(val_clip_img_embeds[:,0,:].numpy(),label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980e9c46-c335-47d1-89fe-4886534be845",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class LinearClassifier(pl.LightningModule):\n",
    "    def __init__(self,n_classes, lr=3e-4):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.lr = lr\n",
    "\n",
    "        self.fc = nn.LazyLinear( n_classes)\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        logits = self.forward(inputs)\n",
    "        loss = self.loss(logits, targets)\n",
    "\n",
    "        preds = torch.argmax(F.softmax(logits, dim=1), dim=1)\n",
    "        acc = torch.sum(preds == targets).item() / targets.size(0)\n",
    "\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        logits = self.forward(inputs)\n",
    "        loss = self.loss(logits, targets)\n",
    "\n",
    "        preds = torch.argmax(F.softmax(logits, dim=1), dim=1)\n",
    "        acc = torch.sum(preds == targets).item() / targets.size(0)\n",
    "\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4833359-497c-442b-9911-c3e6c841bb24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clip_head_classifier=LinearClassifier(n_classes=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1015bb7-fa32-4127-80a4-6e7a84a60627",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clip_features_train_dataset=torch.utils.data.TensorDataset(train_clip_img_embeds[:,0,:],torch.tensor(label_train))\n",
    "clip_features_val_dataset=torch.utils.data.TensorDataset(val_clip_img_embeds[:,0,:],torch.tensor(label_val))\n",
    "clip_features_test_dataset=torch.utils.data.TensorDataset(test_clip_img_embeds[:,0,:],torch.tensor(label_test))\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(clip_features_train_dataset, batch_size=batch, shuffle=True)\n",
    "val_loader = DataLoader(clip_features_val_dataset, batch_size=batch)\n",
    "test_loader = DataLoader(clip_features_test_dataset, batch_size=batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b47d24-094d-4ca3-a844-0f4773683d16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(devices=1,max_epochs=5)\n",
    "trainer.fit(clip_head_classifier, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585c93a8-d442-4c72-8cd0-ad62c273a84d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_input_channels: int, base_channel_size: int, latent_dim: int, act_fn: object = nn.GELU):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "           num_input_channels : Number of input channels of the image. For CIFAR, this parameter is 3\n",
    "           base_channel_size : Number of channels we use in the first convolutional layers. Deeper layers might use a duplicate of it.\n",
    "           latent_dim : Dimensionality of latent representation z\n",
    "           act_fn : Activation function used throughout the encoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(num_input_channels, c_hid, kernel_size=3, padding=1, stride=2),  # 32x32 => 16x16\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, 2 * c_hid, kernel_size=3, padding=1, stride=2),  # 16x16 => 8x8\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1, stride=2),  # 8x8 => 4x4\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1, stride=2),  # 8x8 => 4x4\n",
    "            act_fn(),\n",
    "            nn.Flatten(),  # Image grid to single feature vector\n",
    "            nn.LazyLinear(latent_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "\n",
    "class CNNClassifier(pl.LightningModule):\n",
    "    def __init__(self, cnn_model,teaching_clip_head=None, lr=1e-3,scale=1., distill_knowledge=True):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "\n",
    "        self.cnn_model = cnn_model\n",
    "        self.teaching_clip_head=teaching_clip_head\n",
    "        self.lr = lr\n",
    "        self.scale=scale\n",
    "        self.softmax=torch.nn.Softmax(-1)\n",
    "        self.distill_knowledge=distill_knowledge\n",
    "\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn_model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, features, targets = batch\n",
    "        outputs = self.forward(inputs)\n",
    "        \n",
    "        loss_base = self.loss(outputs,targets)\n",
    "\n",
    "        if self.distill_knowledge:\n",
    "            teacher_outputs=self.teaching_clip_head(features)\n",
    "\n",
    "            outputs_prob=self.softmax(outputs)\n",
    "            teacher_prob=self.softmax(teacher_outputs)\n",
    "\n",
    "            loss_teacher = self.loss(outputs_prob, teacher_prob)*self.scale\n",
    "            loss_base = self.loss(outputs,targets)\n",
    "\n",
    "            loss = loss_base + loss_teacher\n",
    "        else:\n",
    "            loss = loss_base\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        acc = torch.sum(predicted == targets).item() / targets.size(0)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, features, targets = batch\n",
    "        outputs = self.forward(inputs)\n",
    "        \n",
    "        loss_base = self.loss(outputs,targets)\n",
    "\n",
    "        if self.distill_knowledge:\n",
    "            teacher_outputs=self.teaching_clip_head(features)\n",
    "\n",
    "            outputs_prob=self.softmax(outputs)\n",
    "            teacher_prob=self.softmax(teacher_outputs)\n",
    "\n",
    "            loss_teacher = self.loss(outputs_prob, teacher_prob)*self.scale\n",
    "            loss_base = self.loss(outputs,targets)\n",
    "\n",
    "            loss = loss_base + loss_teacher\n",
    "        else:\n",
    "            loss = loss_base\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        acc = torch.sum(predicted == targets).item() / targets.size(0)\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr,weight_decay=1e-2)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2426ca50-2926-4223-98ec-fd147d0201f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Train a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fefb7e-2d24-4043-9637-999e023997e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class KnowledgeClassifierDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,spectro,features,labels,spec_transform=None):\n",
    "        super().__init__()\n",
    "        self.spectro=spectro\n",
    "        self.features=features\n",
    "        self.labels=labels\n",
    "        self.spec_transform=spec_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.spectro)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        \n",
    "        eeg=self.spectro[idx]\n",
    "        feat=self.features[idx]\n",
    "       \n",
    "        if self.spec_transform:\n",
    "            eeg=self.spec_transform(eeg)\n",
    "        \n",
    "        \n",
    "        labels=self.labels[idx]\n",
    "        \n",
    "        return eeg,feat,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e07e50-1866-4b1b-9687-e1e9f4560ff3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_clip_img_embeds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa2a26b-1383-485d-a224-2d8398e2e3d4",
   "metadata": {},
   "source": [
    "## 5. Do Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073e4b57-8190-4156-8936-94981a269339",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subj_train_array=np.array(subject_train)\n",
    "subj_val_array=np.array(subject_val)\n",
    "subj_test_array=np.array(subject_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909198dc-ef6a-456d-8177-d322209d6a26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spectro_train[subj_train_array==subj_idx]\n",
    "# label_train[subj_train_array==subj_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf96413-d040-43c5-b76c-8fcb60e13b9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spec_transforms=torchvision.transforms.Compose([torchvision.transforms.Resize((32,32))])\n",
    "single_subject=True\n",
    "subj_idx=1\n",
    "\n",
    "for subj_idx in [1,2,3,4,5,6]:\n",
    "    \n",
    "    print(f\"[INFO] Processing {subj_idx}\")\n",
    "    if not single_subject:\n",
    "\n",
    "        train_know_dataset = KnowledgeClassifierDataset(spectro_train, train_clip_img_embeds[:,0,:],label_train,spec_transform=spec_transforms)\n",
    "        val_know_dataset = KnowledgeClassifierDataset(spectro_val, val_clip_img_embeds[:,0,:],label_val,spec_transform=spec_transforms)\n",
    "        test_know_dataset = KnowledgeClassifierDataset(spectro_test, test_clip_img_embeds[:,0,:],label_test,spec_transform=spec_transforms)\n",
    "    else:\n",
    "        train_know_dataset = KnowledgeClassifierDataset(spectro_train[subj_train_array==subj_idx], train_clip_img_embeds[:,0,:][subj_train_array==subj_idx],torch.tensor(label_train)[subj_train_array==subj_idx],spec_transform=spec_transforms)\n",
    "        val_know_dataset = KnowledgeClassifierDataset(spectro_val[subj_val_array==subj_idx], val_clip_img_embeds[:,0,:][subj_val_array==subj_idx],torch.tensor(label_val)[subj_val_array==subj_idx],spec_transform=spec_transforms)\n",
    "        test_know_dataset = KnowledgeClassifierDataset(spectro_test[subj_test_array==subj_idx], test_clip_img_embeds[:,0,:][subj_test_array==subj_idx],torch.tensor(label_test)[subj_test_array==subj_idx],spec_transform=spec_transforms)\n",
    "\n",
    "\n",
    "    train_know_dataloader=torch.utils.data.DataLoader(train_know_dataset,batch_size=64,shuffle=True)\n",
    "    val_know_dataloader=torch.utils.data.DataLoader(val_know_dataset,batch_size=64,shuffle=False)\n",
    "\n",
    "    test_know_dataloader=torch.utils.data.DataLoader(test_know_dataset,batch_size=64,shuffle=False)\n",
    "\n",
    "    ## model\n",
    "    \n",
    "    \n",
    "    base_model = nets.Classifier(in_shape=(128,32,32),classes=40,channels=[64,64,128,128,128,128],strides=[2,1,2,1,2,2],kernel_size=3, num_res_units=1, act='GELU', norm=\"BATCH\", dropout=0.)\n",
    "    # base_model=EEGConvClfModel(channels=[128, 128,128])\n",
    "    cnn= CNNClassifier(base_model,teaching_clip_head=clip_head_classifier,lr=3e-4,scale=1,distill_knowledge=True)\n",
    "    summary(cnn, (128,32,32),device=\"cpu\")\n",
    "   \n",
    "    early_stop_callback = EarlyStopping(\n",
    "       monitor='val_acc',\n",
    "       min_delta=0.00,\n",
    "       patience=10,\n",
    "       verbose=False,\n",
    "       mode='max'\n",
    "    )\n",
    "    trainer = Trainer(devices=1,max_epochs=50,callbacks=[early_stop_callback])\n",
    "    trainer.fit(cnn, train_know_dataloader, val_know_dataloader)\n",
    "    trainer.save_checkpoint(f\"models/subj_{subj_idx}.pt\")\n",
    "    \n",
    "    y_pred=[]\n",
    "    y_true=[]\n",
    "    with torch.no_grad():\n",
    "        for spectro, feat, labels in tqdm.tqdm(test_know_dataloader,position=0):\n",
    "            y_pred.append(cnn(spectro))\n",
    "            y_true.append(labels)\n",
    "\n",
    "        y_pred=torch.cat(y_pred)\n",
    "        y_true=torch.cat(y_true)\n",
    "\n",
    "    report=classification_report(y_true.numpy(),y_pred.argmax(-1).numpy(),output_dict=True)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    print(df)\n",
    "    wandb.log({f\"report_{subj_idx}\": df})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d9662b-fe9f-47ce-92a2-34e7e70745b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cnn=Encoder(num_input_channels=128,base_channel_size=64,latent_dim=40)\n",
    "# # cnn=CNNRegressor(cnn,lr=1e-3,scale=1e3)\n",
    "# cnn= CNNClassifier(cnn,teaching_clip_head=clip_head_classifier,lr=3e-4,scale=1)\n",
    "# summary(cnn, (128,32,32),device=\"cpu\")\n",
    "\n",
    "# base_model = nets.Classifier(in_shape=(128,32,32),classes=40,channels=[64,64,128,128,128,128],strides=[2,1,2,1,2,2],kernel_size=3, num_res_units=1, act='GELU', norm=\"BATCH\", dropout=0.1)\n",
    "# # base_model=EEGConvClfModel(channels=[128, 128,128])\n",
    "# cnn= CNNClassifier(base_model,teaching_clip_head=clip_head_classifier,lr=3e-4,scale=1,distill_knowledge=True)\n",
    "# summary(cnn, (128,32,32),device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497bfeeb-c7dc-4e30-86da-b9d27d9a04fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainer = Trainer(devices=1,max_epochs=50)\n",
    "# trainer.fit(cnn, train_know_dataloader, test_know_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61b21ca-7c24-48ab-be69-39005ac1f471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred=[]\n",
    "# y_true=[]\n",
    "# for spectro, feat, labels in tqdm.tqdm(test_know_dataloader,position=0):\n",
    "#     y_pred.append(cnn(spectro))\n",
    "#     y_true.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163de3c5-6d29-4d4c-99ef-9c3415ca121f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# y_pred=torch.cat(y_pred)\n",
    "# y_true=torch.cat(y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b5c753-5c75-4420-b3a9-fc2f64bee54d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# subj_test_array=np.array(subject_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21538a47-18c5-4c28-9317-c6c76bb4d042",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# report=classification_report(y_true.numpy(),y_pred.argmax(-1).numpy(),output_dict=True)\n",
    "# df = pd.DataFrame(report).transpose()\n",
    "# df\n",
    "\n",
    "# wandb.log({f\"report_{subj_idx}\": df})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b613b8a9-4706-4560-9fcc-aa4e837a5915",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306b78e3-a5b8-47d2-8b77-781f749e308e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5B Play a little bit and explore different networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c733a566-ac43-4e0b-bee8-c934a67cf38c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# class EEGConvClfModel(pl.LightningModule):\n",
    "#     def __init__(self, in_channels=128, channels=[128, 256, 384], output_dim=40, do=0.3,lr=1e-4):\n",
    "#         super(EEGConvClfModel, self).__init__()\n",
    "\n",
    "#         self.channels = channels\n",
    "#         self.output_dim = output_dim\n",
    "#         self.num_layers = len(channels)\n",
    "#         self.lr = lr\n",
    "#         self.validation_step_outputs = []\n",
    "#         self.train_step_outputs = []\n",
    "\n",
    "#         conv_layers = []\n",
    "#         seq_length = 768\n",
    "\n",
    "#         for i in range(self.num_layers):\n",
    "#             out_channels = channels[i]\n",
    "#             conv_layers.extend([\n",
    "#                 nn.Conv1d(in_channels, out_channels, kernel_size=12, stride=4, padding=1),\n",
    "#                 nn.GELU(),\n",
    "#                 nn.BatchNorm1d(out_channels)\n",
    "#             ])\n",
    "#             in_channels = out_channels\n",
    "#             seq_length //= 2\n",
    "\n",
    "#         self.conv_layers = nn.Sequential(*conv_layers,nn.Dropout(do))\n",
    "        \n",
    "        \n",
    "        \n",
    "#         self.fc_layers = nn.LazyLinear(output_dim)\n",
    "\n",
    "#         self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "#         self.train_epoch_loss = []  # List to store training loss values per epoch\n",
    "#         self.val_epoch_loss = []    # List to store validation loss values per epoch\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv_layers(x)\n",
    "#         x = torch.flatten(x, start_dim=1)\n",
    "#         x = self.fc_layers(x)\n",
    "#         return x\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         inputs, targets = batch\n",
    "#         outputs = self.forward(inputs)\n",
    "#         loss = self.loss(outputs, targets)\n",
    "        \n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "#         acc = torch.sum(predicted == targets).item() / targets.size(0)\n",
    "#         self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "#         self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "#         # self.train_step_outputs.append(loss)\n",
    "\n",
    "#         return loss\n",
    "\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         inputs, targets = batch\n",
    "#         outputs = self.forward(inputs)\n",
    "#         loss = self.loss(outputs, targets)\n",
    "        \n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "#         acc = torch.sum(predicted == targets).item() / targets.size(0)\n",
    "#         self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "#         self.log('val_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "#         # self.validation_step_outputs.append(loss)\n",
    "\n",
    "#         return loss\n",
    "\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         optimizer = torch.optim.AdamW(self.parameters(), self.lr, weight_decay=1e-2)\n",
    "#         lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5)\n",
    "#         return {\n",
    "#             \"optimizer\": optimizer,\n",
    "#             \"lr_scheduler\": lr_scheduler,\n",
    "#             \"monitor\": \"val_loss\"\n",
    "#         }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c942c4ed-d539-411f-8129-0ebd1d4ca07b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spec_transforms=torchvision.transforms.Compose([torchvision.transforms.Resize((32,32))])\n",
    "# # spec_transforms=None\n",
    "# single_subject=True\n",
    "# subj_idx=1\n",
    "\n",
    "# if not single_subject:\n",
    "    \n",
    "#     train_know_dataset = KnowledgeClassifierDataset(spectro_train, train_clip_img_embeds[:,0,:],label_train,spec_transform=spec_transforms)\n",
    "#     test_know_dataset = KnowledgeClassifierDataset(spectro_test, test_clip_img_embeds[:,0,:],label_test,spec_transform=spec_transforms)\n",
    "# else:\n",
    "#     train_know_dataset = KnowledgeClassifierDataset(spectro_train[subj_train_array==subj_idx], train_clip_img_embeds[:,0,:][subj_train_array==subj_idx],torch.tensor(label_train)[subj_train_array==subj_idx],spec_transform=spec_transforms)\n",
    "#     test_know_dataset = KnowledgeClassifierDataset(spectro_test[subj_test_array==subj_idx], test_clip_img_embeds[:,0,:][subj_test_array==subj_idx],torch.tensor(label_test)[subj_test_array==subj_idx],spec_transform=spec_transforms)\n",
    "\n",
    "\n",
    "# train_know_dataloader=torch.utils.data.DataLoader(train_know_dataset,batch_size=64,shuffle=True)\n",
    "# test_know_dataloader=torch.utils.data.DataLoader(test_know_dataset,batch_size=64,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df22f3b-7f7e-4e80-a8cb-2594347dc5fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# s,f,c=next(iter(train_know_dataloader))\n",
    "# print(*s.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bacbb4b-a9db-44e5-9b10-887f3a548380",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# base_model = nets.Classifier(in_shape=(128,32,32),classes=40,channels=[64,64,128,128],strides=[2,2,2,2],kernel_size=5, num_res_units=1, act='GELU', norm='BATCH', dropout=0.3)\n",
    "# # base_model=EEGConvClfModel(channels=[128, 128,128])\n",
    "# cnn= CNNClassifier(base_model,teaching_clip_head=clip_head_classifier,lr=3e-4,scale=1,distill_knowledge=True)\n",
    "# summary(cnn, (128,32,32),device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04efec71-10eb-4262-af84-0be59144cf8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainer = Trainer(devices=1,max_epochs=20)\n",
    "# trainer.fit(cnn, train_know_dataloader, test_know_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa40736-b925-4ab4-a9ef-6db408a980d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5c. Masked Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb79890-8e14-4b62-8efd-d6abd14a4c97",
   "metadata": {},
   "source": [
    "## 6. Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c168070f-04b0-4ada-8187-24d21546b361",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def convert_synsets_to_names(synsets):\n",
    "    nltk.download('wordnet')  # Download WordNet corpus if not already downloaded\n",
    "\n",
    "    class_names = []\n",
    "    for synset_id in synsets:\n",
    "        synset = wn.synset_from_pos_and_offset(synset_id[0], int(synset_id[1:]))\n",
    "        class_name = synset.lemmas()[0].name()\n",
    "        class_names.append(class_name)\n",
    "\n",
    "    return class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c08e52-37ca-472b-b7dd-1811483bd70b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_names=convert_synsets_to_names(data[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609eb69b-0b08-464d-814b-567b262821c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "syn_to_class = {k:v for k,v in zip(range(40),class_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5bbb8e-0458-42f8-86df-5f82baeaf0d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "syn_to_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164ef9b9-48e0-4622-9e50-99ef909bbe7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx=15\n",
    "plt.imshow(Image.open(img_test[idx]))\n",
    "plt.title(syn_to_class[label_test[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dcaf81-75cf-4cb4-8d76-5a38d1a1e95f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(preds, true_labels):\n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(true_labels, np.argmax(preds, axis=1))\n",
    "\n",
    "    # Get the indices of the top 5 predicted classes for each sample\n",
    "    top3_indices = np.argsort(preds, axis=1)[:, -3:]\n",
    "\n",
    "    # Check if the true label is present in the top 5 predicted classes\n",
    "    top3_accuracy = np.mean(np.any(top3_indices == true_labels.reshape(-1, 1), axis=1))\n",
    "\n",
    "\n",
    "    # Get the indices of the top 5 predicted classes for each sample\n",
    "    top5_indices = np.argsort(preds, axis=1)[:, -5:]\n",
    "\n",
    "    # Check if the true label is present in the top 5 predicted classes\n",
    "    top5_accuracy = np.mean(np.any(top5_indices == true_labels.reshape(-1, 1), axis=1))\n",
    "\n",
    "    # Compute F1 score\n",
    "    f1 = f1_score(true_labels, np.argmax(preds, axis=1), average='macro')\n",
    "\n",
    "    # Compute precision\n",
    "    precision = precision_score(true_labels, np.argmax(preds, axis=1), average='macro')\n",
    "\n",
    "    # Compute recall\n",
    "    recall = recall_score(true_labels, np.argmax(preds, axis=1), average='macro')\n",
    "\n",
    "    return accuracy, top3_accuracy,top5_accuracy, f1, precision, recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7eb7d6-924c-4bb3-9772-a4d460a2b0dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subj_idx=1\n",
    "metrics={}\n",
    "for subj_idx in range(1,7):\n",
    "    \n",
    "    print(f\"processing {subj_idx}\")\n",
    "    base_model = nets.Classifier(in_shape=(128,32,32),classes=40,channels=[64,64,128,128,128,128],strides=[2,1,2,1,2,2],kernel_size=3, num_res_units=1, act='GELU', norm=\"BATCH\", dropout=0.)\n",
    "    model= CNNClassifier(base_model,teaching_clip_head=clip_head_classifier,lr=3e-4,scale=1,distill_knowledge=True)\n",
    "    model.load_state_dict(torch.load(f\"models/subj_{subj_idx}.pt\")[\"state_dict\"])\n",
    "    \n",
    "    inputs= torch.stack([spec_transforms(i) for i in spectro_test[subj_test_array==subj_idx]])\n",
    "    with torch.no_grad():\n",
    "        preds=model(inputs)\n",
    "    \n",
    "    metric_values=compute_metrics(preds.numpy(),np.array(label_test)[subj_test_array==subj_idx])\n",
    "    metrics[f\"subj_{subj_idx}\"]={\"accuracy\":metric_values[0],\n",
    "                                 \"top-3_accuracy\":metric_values[1],\n",
    "                                 \"top-5_accuracy\":metric_values[2],\n",
    "                                 \"f1\":metric_values[3],\n",
    "                                 \"precision\":metric_values[4],\n",
    "                                 \"recall\":metric_values[5],\n",
    "                                 \"preds\":preds.argmax(-1).numpy(),\n",
    "                                 \"labels\":np.array(label_test)[subj_test_array==subj_idx]\n",
    "                                }\n",
    "                                 \n",
    "                                 \n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7635fb33-716c-48b4-8148-7fc7e73d6b92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame.from_dict(metrics)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf70c00-3346-4e59-9903-3875565fbfd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_metrics=df.iloc[:5].transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a147bc5f-96c3-4bb5-b437-67ab78522395",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_metrics.to_csv(\"report.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e99cb8c-59ca-44b4-b886-7ee0746752ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Set a theme\n",
    "custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n",
    "sns.set_theme(style=\"ticks\", rc=custom_params)\n",
    "# Create a figure and axes\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Create a barplot\n",
    "\n",
    "sns.barplot( data=df_metrics.sort_values(by=\"subj_4\",axis=1, ascending=False), palette='viridis', ax=ax,alpha=0.9)\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Metric', fontsize=14)\n",
    "ax.set_ylabel('Value', fontsize=14)\n",
    "ax.set_title('CLIP-Guided EEG Decoding Performances', fontsize=18)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "plt.savefig(\"figures/performances.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4849155f-ac14-48fe-9493-60afe25e5665",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "report=classification_report(np.array(label_test)[subj_test_array==subj_idx],preds.argmax(-1).numpy(),output_dict=True,target_names=class_names)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d7ec75-a3ea-4e35-aad1-f74639b1aa25",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7. Prediction to images - just separate boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e86cac7-6fa6-4039-9dd5-22801220246b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
    "import torch\n",
    "\n",
    "repo_id = \"stabilityai/stable-diffusion-2-base\"\n",
    "pipe = DiffusionPipeline.from_pretrained(repo_id, torch_dtype=torch.float16, revision=\"fp16\")\n",
    "\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe = pipe.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c901b38b-87e9-40fe-905c-a3981d764156",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_test_subj=np.array(img_test)[subj_test_array==subj_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871f3dc2-c248-4643-a508-550f5bb9e08f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics[\"subj_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e928f70-97af-41d3-80b8-95330088415b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "decoded_images={}\n",
    "test_images={}\n",
    "for subj_idx in range(1,7):\n",
    "    print(f\"processing {subj_idx}\")\n",
    "    img_test_subj=np.array(img_test)[subj_test_array==subj_idx]\n",
    "    key=f\"subj_{subj_idx}\"\n",
    "    \n",
    "    subj_imgs=[]\n",
    "    for p in metrics[key][\"preds\"]:\n",
    "        prompt = f\"High quality photo of {syn_to_class[p]}\"\n",
    "        image = pipe(prompt, num_inference_steps=40,guidance_scale=9,negative_prompt=\"bad quality,poor resolution\").images[0]\n",
    "        subj_imgs.append(image)\n",
    "    decoded_images[key]=subj_imgs\n",
    "    test_images[key]=[Image.open(i) for i in img_test_subj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103f0150-0649-41b2-b82b-6c7f106605e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.array(decoded_images[f\"subj_1\"]).__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94590469-e9b9-4496-89a9-633f8f46c299",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "offset=0\n",
    "subj_idx=6\n",
    "\n",
    "# selected_class=10\n",
    "\n",
    "for selected_class in tqdm.tqdm(range(40),position=0):\n",
    "    \n",
    "    indices=np.array([metrics[f\"subj_{subj_idx}\"][\"labels\"]==selected_class]).squeeze()\n",
    "    \n",
    "    if indices.sum()>5:\n",
    "        nrow=5\n",
    "    else:\n",
    "        nrow= indices.sum()\n",
    "    fig, axs = plt.subplots(nrow,6,figsize=(20,20))\n",
    "\n",
    "    for i in range(nrow):\n",
    "        axs[i,0].imshow(np.array(test_images[f\"subj_{subj_idx}\"])[indices][i+offset])\n",
    "        axs[i,0].axis(\"off\")\n",
    "        for j in range(2,7):\n",
    "            axs[i,j-1].imshow(np.array(decoded_images[f\"subj_{j}\"])[indices][i+offset])\n",
    "            axs[i,j-1].axis(\"off\")\n",
    "    axs[0,0].set_title(\"stimulus\")\n",
    "    for j in range(2,7):\n",
    "        axs[0,j-1].set_title(f\"subj_{j}\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"figures/{class_names[selected_class]}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bfc3f2-a11e-4a3b-8233-837eb7766fe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7747c3-894e-4c8b-bf95-7f00df7be29b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.array([metrics[f\"subj_{subj_idx}\"][\"labels\"]==selected_class])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098b30ca-cbee-4326-b8ac-20a7fca24318",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.array(test_images[f\"subj_{subj_idx}\"])[np.array([metrics[f\"subj_{subj_idx}\"][\"labels\"]==selected_class]).squeeze()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800667cf-130d-4781-bc0c-a50965d9aea9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs=plt.subplots(1,2,figsize=(20,10))\n",
    "\n",
    "axs[0].imshow(Image.open(img_test_subj[idx]))\n",
    "axs[1].imshow(image)\n",
    "\n",
    "axs[0].axis(\"off\")\n",
    "axs[1].axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6aefb9-fe6b-4c2d-8938-53ed61073251",
   "metadata": {},
   "source": [
    "## 6B Predict on averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcf580a-20f5-4c42-b64e-c5bba649ae64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b5d976-1aba-4b73-8948-81b2ef7dcc39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out=pipe.vae.decode(z).sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857e9734-11b4-4497-adaf-aaa2d7df6726",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "z.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963fa2b9-8bc5-48d0-aa2e-99127e5d67c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# out=top(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986ec420-8e20-4b42-9eaf-05a1b802267a",
   "metadata": {},
   "source": [
    "## 7. Extract latent from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d36873-6325-474c-8857-a44905efec71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Create the normalization transform\n",
    "normalize_transform = torchvision.transforms.Normalize(mean, std)\n",
    "\n",
    "tot=torchvision.transforms.Compose([torchvision.transforms.Resize((64,64)),torchvision.transforms.ToTensor()])\n",
    "top=torchvision.transforms.ToPILImage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55c4641-d7be-4d40-9b15-81b7011a9640",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "latents_train=[]\n",
    "with torch.no_grad():\n",
    "    for im in tqdm.tqdm(img_train,position=0):\n",
    "        im=tot(Image.open(im).convert(\"RGB\"))\n",
    "        z=pipe.vae.encode(torch.tensor(im).unsqueeze(0).half().cuda()).latent_dist.mode()\n",
    "        latents_train.append(z.cpu())\n",
    "    latents_train= torch.cat(latents_train).flatten(start_dim=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e450cdf-9830-4777-9414-9d3269d465cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "latents_val=[]\n",
    "with torch.no_grad():\n",
    "    for im in tqdm.tqdm(img_val,position=0):\n",
    "        im=tot(Image.open(im).convert(\"RGB\"))\n",
    "        z=pipe.vae.encode(torch.tensor(im).unsqueeze(0).half().cuda()).latent_dist.mode()\n",
    "        latents_val.append(z.cpu())\n",
    "    latents_val= torch.cat(latents_val).flatten(start_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eb5d28-13ae-4c9f-8bc2-76b90cede6e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "latents_test=[]\n",
    "with torch.no_grad():\n",
    "    for im in tqdm.tqdm(img_test,position=0):\n",
    "        im=tot(Image.open(im).convert(\"RGB\"))\n",
    "        z=pipe.vae.encode(torch.tensor(im).unsqueeze(0).half().cuda()).latent_dist.mode()\n",
    "        latents_test.append(z.cpu())\n",
    "    latents_test= torch.cat(latents_test).flatten(start_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d7334d-f7b1-4bfd-9fb7-d4278ae7e0fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_extractor=copy.deepcopy(model.cnn_model)\n",
    "feature_extractor.final=nn.Flatten()\n",
    "feature_extractor.reshape=nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982d5771-8c1f-4c3e-acea-3473cce139c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spectro_resized_subj_train= torch.stack([spec_transforms(i) for i in spectro_train[subj_train_array==subj_idx]])\n",
    "with torch.no_grad():\n",
    "    eeg_features_train=feature_extractor(spectro_resized_subj_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa842609-e280-447b-ad8b-1acb34c9742d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spectro_resized_subj_test= torch.stack([spec_transforms(i) for i in spectro_test[subj_test_array==subj_idx]])\n",
    "with torch.no_grad():\n",
    "    eeg_features_test=feature_extractor(spectro_resized_subj_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f74e19-c55c-4c28-adbd-5641a94b7c6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# regressor=RidgeCV(alphas=[1,10,100,1000,1e4,1e5],fit_intercept=True)\n",
    "regressor=Ridge(alpha=0.1,fit_intercept=True)\n",
    "\n",
    "regressor.fit(eeg_features_train.numpy(),latents_train[subj_train_array==subj_idx].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdc4737-414e-46ec-95fc-281d2cc2bbbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds=torch.tensor(regressor.predict(eeg_features_train.numpy())).reshape(-1,4,8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed7fc7f-b650-4a58-be0a-27386560d580",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    a=pipe.vae.decode(preds[:10].cuda().half()).sample.cpu().float()\n",
    "    b=pipe.vae.decode(latents_train[subj_train_array==subj_idx][:10].reshape(-1,4,8,8).cuda().half()).sample.cpu().float()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d83956d-252e-4463-984b-953b9cccf8a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig,axs=plt.subplots(3,2)\n",
    "\n",
    "for i in range(3):\n",
    "    axs[i,0].imshow(top(a[i]))\n",
    "    axs[i,1].imshow(top(b[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8664e2f5-d54c-4b9c-90df-b24281b794d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 9. Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64956e53-11c4-4a41-a27c-7a2617da0ede",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (Optional) Masked autoencoder for EEG data, neural latents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b52712-de7a-4a7c-a803-a7edbe85e448",
   "metadata": {},
   "source": [
    "##  (Optional) Cebra for dimensionality reduction with CLIP latents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7135f087-5253-4a59-a9fc-85ce690f8bb1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 8. Try to regress out latents from EEG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a5546a-b785-43bf-a021-5d2a76abbf8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171691f9-3552-4b52-ab80-69cba69b9896",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eeg_features_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f1292b-7d43-4bd9-8bfb-78aaf5e16383",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "regressor=RidgeCV(alphas=[1,10,100,1000,1e4],fit_intercept=False)\n",
    "regressor.fit(train_clip_img_embeds[:,0,:][subj_train_array==subj_idx].numpy(),eeg_features_train.numpy(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776447cd-985f-45ec-b7aa-6ab8be73e79f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inverse_regressor=np.linalg.pinv(regressor.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be125dc-f28e-4761-ade3-be168f1d93b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimated_clip_from_eeg_test_subj=eeg_features_test.numpy()@inverse_regressor.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a44a25-0ece-43d0-a40f-e2cd43267489",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds_from_estimated=clip_head_classifier(torch.tensor(estimated_clip_from_eeg_test_subj).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a896f642-7469-4fbc-b0bc-ee1e0082702b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "report=classification_report(np.array(label_test)[subj_test_array==subj_idx],preds_from_estimated.argmax(-1).numpy(),output_dict=True,target_names=class_names)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "braindiff",
   "language": "python",
   "name": "braindiff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
